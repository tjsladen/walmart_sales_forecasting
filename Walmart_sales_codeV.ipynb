{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87aeb57b-92de-4ab2-92da-d6a60625a59b",
   "metadata": {},
   "source": [
    "# Forecasting Weekly Sales Across Walmart Stores Using Prophet\n",
    "\n",
    "## The Problem at Hand\n",
    "\n",
    "The purpose of this project is to forecast future sales predictions for forty five different Walmart stores in the USA and to determine if additional features may prove most useful to support this. Why is forecasting useful? Forecasting can provide insights to aid logistical management of stores for example, stock and staff planning to ensure stock meets demand whilst ensuring the store is neither under or over staffed to maintain cost effectiveness. \n",
    "\n",
    "The dataset used for this project comes from Kaggle authored by M Yasser H and can be found by following this link: [Walmart Dataset](https://www.kaggle.com/datasets/yasserh/walmart-dataset). The time span of this data ranges from 5/2/2010 to 1/11/2012 where sales data is represented weekly. Additional variables within this dataset include: Store (Store number), Holiday_Flag (Binary, 1 if holiday occured during a week), Temperature (Fahrenheit), Fuel_Price(US$ per gallon), CPI (Consumer price index)and Unemployment (Unemployment rate). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544a0ba-b94d-4b6e-91ab-79048dd70940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\tjsla\\OneDrive\\Desktop\\Personal projects\\Walmart sales forecasting\\scripts_functions\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dcf_test_1 import check_zeros_nas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473eba66-1d34-4e06-b1e9-6a6b26814fbf",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2731a-61f6-4bef-bb86-7521ce1aacb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_Walmart = pd.read_csv(\"C:/Users/tjsla/OneDrive/Desktop/Personal projects/Walmart sales forecasting/Data/Walmart.csv\")\n",
    "Full_Walmart[\"Date\"] = pd.to_datetime(Full_Walmart[\"Date\"],format=\"%d-%m-%Y\")\n",
    "\n",
    "stores = Full_Walmart['Store'].unique()\n",
    "store_dfs = {store: Full_Walmart[Full_Walmart['Store'] == store][['Date', 'Weekly_Sales']] for store in stores}\n",
    "\n",
    "for store in stores:\n",
    "    df = store_dfs[store].copy()\n",
    "    df = df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y', 'cap': 'cap'})\n",
    "    store_dfs[store] = df\n",
    "\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "store_errors = {}\n",
    "\n",
    "for store in stores:\n",
    "    df = store_dfs[store].copy()\n",
    "    df = df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})\n",
    "    df['ds'] = pd.to_datetime(df['ds'], dayfirst=True, errors='coerce') \n",
    "    df = df.dropna(subset=['ds', 'y']) \n",
    "    store_dfs[store] = df\n",
    "\n",
    "combined_df = pd.concat(store_dfs.values(), keys=store_dfs.keys(), names=['Store'])\n",
    "combined_df = combined_df.reset_index()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.boxplot(x='Store', y='y', data=combined_df)\n",
    "plt.title('Weekly Sales Distribution per Store')\n",
    "plt.ylabel('Weekly Sales (Millions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff545d68-dced-4330-b0b1-83e18c517759",
   "metadata": {},
   "source": [
    "<p><b>Figure 1:</b> Boxplot of weekly sale ranges for each store.</p>\n",
    "\n",
    "\n",
    "The initial exploration involved understanding the sales distributions for each store to determine if there are any patterns or unique cases. Figure 1 provides information on the sale ranges for all 45 stores where, a key detail is the presence of outliers for many stores typically representing high sale volumes. Usually the reasons for higher sales stem from a number of factors such as promotions and holiday periods, in this case the data only provided information for holiday events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b233356-33a3-438e-b2ad-9c1b04424293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "Full_Walmart['Year'] = Full_Walmart['Date'].dt.year\n",
    "\n",
    "sales_by_holiday_year = (\n",
    "    Full_Walmart.groupby(['Holiday_Flag', 'Year'])['Weekly_Sales']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "sales_by_holiday_year['Holiday'] = sales_by_holiday_year['Holiday_Flag'].map({0: 'Non-Holiday', 1: 'Holiday'})\n",
    "\n",
    "pivot_df = sales_by_holiday_year.pivot(index='Year', columns='Holiday', values='Weekly_Sales')\n",
    "percent_df = pivot_df.div(pivot_df.sum(axis=1), axis=0) * 100\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for label, color in zip(['Non-Holiday', 'Holiday'], ['skyblue', 'green']):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=pivot_df.index.astype(str),\n",
    "            y=pivot_df[label],\n",
    "            name=label,\n",
    "            marker_color=color,\n",
    "            customdata=percent_df[label].values.reshape(-1, 1),\n",
    "            hovertemplate=(\n",
    "                \"<b>\" + label + \"</b><br>\" +\n",
    "                \"Year: %{x}<br>\" +\n",
    "                \"Sales: %{y:,}<br>\" +\n",
    "                \"Percent: %{customdata[0]:.1f}%<extra></extra>\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Total Weekly Sales by Year: Holiday vs Non-Holiday',\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='Total Sales',\n",
    "    legend_title='Week Type',\n",
    "    template='plotly_white',\n",
    "    height=600,\n",
    "    width = 600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390fa68-8746-403b-92ec-30200958da68",
   "metadata": {},
   "source": [
    "<p><b>Figure 2:</b> Stacked plot of total sales across all stores by year for holiday and non-holiday weeks (Interactable).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666f6f1-4a87-4235-9f12-870dcd6c6e11",
   "metadata": {},
   "source": [
    "Figure 2 provides insight into the total sales for each year and the proportion of which occurred during holiday periods. For each year the percentage of sales which fell into holiday weeks were as follows: [8.7%],[8.4%] and [4.9%]. Although 2012 experienced the least number of total and holiday sales, this is likely explained by data ending on 26/10/2012 where notably the Christmas period is missing for this year which may contribute to the lower values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59c7e4-4249-43e1-9edf-ad8860ca21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_totals = Full_Walmart.groupby(\"Store\")[\"Weekly_Sales\"].sum().reset_index()\n",
    "store_totals.columns = [\"Store\", \"Total_Sales\"]\n",
    "\n",
    "store_totals[\"Performance_Tier\"] = pd.qcut(\n",
    "    store_totals[\"Total_Sales\"], \n",
    "    q=3,\n",
    "    labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "Full_Walmart = Full_Walmart.merge(store_totals[[\"Store\", \"Performance_Tier\"]], on=\"Store\")\n",
    "\n",
    "high_perf = Full_Walmart[Full_Walmart[\"Performance_Tier\"] == \"High\"]\n",
    "medium_perf = Full_Walmart[Full_Walmart[\"Performance_Tier\"] == \"Medium\"]\n",
    "low_perf = Full_Walmart[Full_Walmart[\"Performance_Tier\"] == \"Low\"]\n",
    "\n",
    "bright_13_palette = [\n",
    "    \"#e6194b\",  # red\n",
    "    \"#3cb44b\",  # green\n",
    "    \"#ffe119\",  # yellow\n",
    "    \"#4363d8\",  # blue\n",
    "    \"#f58231\",  # orange\n",
    "    \"#911eb4\",  # purple\n",
    "    \"#46f0f0\",  # cyan\n",
    "    \"#f032e6\",  # magenta\n",
    "    \"#bcf60c\",  # lime\n",
    "    \"#fabebe\",  # pink\n",
    "    \"#008080\",  # teal\n",
    "    \"#e6beff\",  # lavender\n",
    "]\n",
    "yticks_hp = np.arange(1000000, 4000000 + 1, 500000)\n",
    "\n",
    "hp_plotly = px.line(\n",
    "    high_perf,\n",
    "    x=\"Date\",\n",
    "    y=\"Weekly_Sales\",\n",
    "    color=\"Store\",\n",
    "    title=\"High Performance Store Sales\",\n",
    "    labels={\n",
    "        \"Date\": \"Date\",\n",
    "        \"Weekly_Sales\": \"Weekly Sales\",\n",
    "        \"Store\": \"Store Number\"\n",
    "    },\n",
    "    color_discrete_sequence=bright_13_palette, \n",
    "    template = \"plotly_white\"\n",
    ")\n",
    "\n",
    "hp_plotly.update_yaxes(tickformat=\",\", title=\"Weekly Sales\")\n",
    "\n",
    "hp_plotly.update_xaxes(\n",
    "    dtick=\"M1\", \n",
    "    tickformat=\"%b\\n%Y\",\n",
    "    title=\"Date\"\n",
    ")\n",
    "\n",
    "hp_plotly.update_yaxes(\n",
    "    tickformat=\",\",\n",
    "    tickvals=yticks_hp,\n",
    "    title=\"Weekly Sales\"\n",
    ")\n",
    "hp_plotly.update_traces(line=dict(width=1))\n",
    "\n",
    "\n",
    "hp_plotly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1022c-35ff-41bd-b592-1290a77d91e8",
   "metadata": {},
   "source": [
    "<p><b>Figure 3:</b> Lineplot of weekly sales for upper 33% performance stores (Interactable).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b68cf-d006-4958-9820-a99398c2756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yticks_mp = np.arange(500000, 2500000 + 1, 250000)\n",
    "\n",
    "mp_plotly = px.line(\n",
    "    medium_perf,\n",
    "    x=\"Date\",\n",
    "    y=\"Weekly_Sales\",\n",
    "    color=\"Store\",\n",
    "    title=\"Medium Performance Store Sales\",\n",
    "    labels={\n",
    "        \"Date\": \"Date\",\n",
    "        \"Weekly_Sales\": \"Weekly Sales\",\n",
    "        \"Store\": \"Store Number\"\n",
    "    },\n",
    "    color_discrete_sequence=bright_13_palette,  \n",
    "    template = \"plotly_white\"\n",
    ")\n",
    "\n",
    "mp_plotly.update_yaxes(tickformat=\",\", title=\"Weekly Sales\")\n",
    "\n",
    "\n",
    "mp_plotly.update_xaxes(\n",
    "    dtick=\"M1\",  \n",
    "    tickformat=\"%b\\n%Y\",  \n",
    "    title=\"Date\"\n",
    ")\n",
    "\n",
    "mp_plotly.update_yaxes(\n",
    "    tickformat=\",\",\n",
    "    tickvals=yticks_mp,\n",
    "    title=\"Weekly Sales\"\n",
    ")\n",
    "mp_plotly.update_traces(line=dict(width=1))\n",
    "\n",
    "mp_plotly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629cbaa-3fb4-4383-945f-f0cbe57a8108",
   "metadata": {},
   "source": [
    "<p><b>Figure 4:</b> Lineplot of weekly sales for middle 33% performance stores (Interactable).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35f8a6-43bf-4b5f-b665-82bb7d27bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "yticks_lp = np.arange(200000, 1400000 + 1, 200000)\n",
    "\n",
    "lp_plotly = px.line(\n",
    "    low_perf,\n",
    "    x=\"Date\",\n",
    "    y=\"Weekly_Sales\",\n",
    "    color=\"Store\",\n",
    "    title=\"Low Performance Store Sales\",\n",
    "    labels={\n",
    "        \"Date\": \"Date\",\n",
    "        \"Weekly_Sales\": \"Weekly Sales\",\n",
    "        \"Store\": \"Store Number\"\n",
    "    },\n",
    "    color_discrete_sequence=bright_13_palette,  \n",
    "    template = \"plotly_white\"\n",
    ")\n",
    "\n",
    "lp_plotly.update_yaxes(tickformat=\",\", title=\"Weekly Sales\")\n",
    "\n",
    "lp_plotly.update_xaxes(\n",
    "    dtick=\"M1\", \n",
    "    tickformat=\"%b\\n%Y\",  \n",
    "    title=\"Date\"\n",
    ")\n",
    "\n",
    "lp_plotly.update_yaxes(\n",
    "    tickformat=\",\",\n",
    "    tickvals=yticks_lp,\n",
    "    title=\"Weekly Sales\"\n",
    ")\n",
    "lp_plotly.update_traces(line=dict(width=1))\n",
    "\n",
    "lp_plotly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f92bd-80f3-41a0-a247-592f73015b08",
   "metadata": {},
   "source": [
    "<p><b>Figure 5:</b> Lineplot of weekly sales for lower 33% performance stores (Interactable).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5fc859-1240-4a0b-a36f-5ca7ca17cadd",
   "metadata": {},
   "source": [
    "Figures 3-5 provide insight into store sale trends over time where stores had been divided into three groups based on a three quartile split. High performing stores consist of the top 33% performing stores based on sales, medium performance the middle 33% and low performance on the lowest 33% of stores. Figure 3 implied top performing stores typically shared similar patterns with major peaks around Christmas. Similarly medium performance stores also follow the same trend however, stores 28 and 41 notably performed better than other stores during non Christmas periods suggesting they may have been better grouped within the high performance stores. Additionally, store 18 saw decreased sales during September 2011 later recovering in October/November implying an event occurred during this period however, the cause is unknown. Finally, low performance stores also typically share the same pattern with some exceptions. For example, store 33 notably does not see as dramatic an increase in sales during the Christmas period with sales remaining roughly stable throughtout the time period comparatively, store 36 implied a decrease in sales overtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c3a792-d2e2-4c67-b7cf-ffcfdab1a923",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Following the exploratory analysis, although prophet is a time series based model approach and therefore does not typically use additional features, feature importance was still utilised to determine if any additional features would improve model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10511294-1aad-4f8c-833d-1be5879daa1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_features = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "corr = Full_Walmart[numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ccb72-be88-4999-935d-fb51a3094088",
   "metadata": {},
   "source": [
    "<p><b>Figure 6:</b> Correlation matrix of numerical features and their relationship with sales.</p>\n",
    "Figure 6 implied little relationship between the features available with the strongest relationship being between CPI and Unemployment [-0.30] which implied a minor negative relationship. This implied that no features present had a strong relationship with weekly sales suggesting they may be unnecessary for the prophet model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd21bc-5958-4a98-a60c-2971dc07db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Holiday_Flag']\n",
    "X = Full_Walmart[features]\n",
    "y = Full_Walmart['Weekly_Sales']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb = XGBRegressor(random_state=42,enable_categorical=True)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test,show=False)\n",
    "ax = plt.gca()\n",
    "\n",
    "max_val = ax.get_xlim()[1]\n",
    "xticks = np.arange(-900000, max_val + 1, 300000)\n",
    "ax.set_xticks(xticks)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02d096-eb4d-4c89-aa26-2d11aef5a4b1",
   "metadata": {},
   "source": [
    "p><b>Figure 7:</b> Beeswarm plot of SHAP values for additional features.</p>\n",
    "Additionally, figure 7 provides further insight into feature impact on weekly sales. In this case feature effects were evaluated using an XGBoost model to invetsigate effects on model accuracy. The SHAP values indicated that unemployment and CPI generally had the greatest impact on weekly sales whilst holiday flags had minimal impact with higher values potentially being related to the Christmas periods specifically. However, it should be noted that the direction of each variable is somewhat ambigous although, unemployment and CPI generally suggested higher values would negatively impact sales. Overall, due to a lack of clear contributions no additional features were implemented into the prophet model to avoid reductions in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a340a0-512e-4892-b58b-7ff69cb757a6",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "In order to prepare the data for modelling the first step involved checking for missing values. This was conducted using a function called check_zeros_nas which totalled NA and 0 value recordings for each column and storing the results. The output of this function implied no missing values or unexpected 0 recordings were present. Following this the \"date\" variable was formatted to ensure it was in the correct date-time format before being passed to prophet. \n",
    "\n",
    "As mentioned additional features were tested using an XGBoost model. This was conducted by running the XGBoost regression model using all available features and then applying the results to both permutation importance and shap algorithms. \n",
    "\n",
    "Before conducting forecasting a cross validation setup (prophet uses rolling-origin evaluation) was applied to the initial model using a setup of an initial 730 days, 90 day period and 90 day horizon. This cross validation was used to evaluate how well the model forecasts future data alongside tuning the changepoint paramater.\n",
    "\n",
    "Forecasting was then conducted using the same setup as it proved effective, alongside a changepoint value of 0.5 being utilised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a7ec5-4b8c-499a-ba65-bae927516942",
   "metadata": {},
   "source": [
    "## Forecasting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657170d-082f-4644-a887-e01b5f752ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"cmdstanpy\").setLevel(logging.WARNING)\n",
    "\n",
    "stores = Full_Walmart['Store'].unique()\n",
    "store_dfs = {store: Full_Walmart[Full_Walmart['Store'] == store][['Date', 'Weekly_Sales']] for store in stores}\n",
    "\n",
    "for store in stores:\n",
    "    df = store_dfs[store].copy()\n",
    "    df = df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y', 'cap': 'cap'})\n",
    "    store_dfs[store] = df\n",
    "\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "store_errors = {}\n",
    "\n",
    "for store in stores:\n",
    "    df = store_dfs[store].copy()\n",
    "    df = df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})\n",
    "    df['ds'] = pd.to_datetime(df['ds'], dayfirst=True, errors='coerce')  \n",
    "    df = df.dropna(subset=['ds', 'y'])  \n",
    "    store_dfs[store] = df\n",
    "\n",
    "for store in stores:\n",
    "    df = store_dfs[store]\n",
    "    model = Prophet(yearly_seasonality=True, weekly_seasonality=True,\\\n",
    "                    changepoint_prior_scale=0.5)    \n",
    "    model.fit(df)\n",
    "\n",
    "    try:\n",
    "        df_cv = cross_validation(model, initial='730 days', period='90 days', horizon='90 days', parallel=\"processes\")\n",
    "        df_p = performance_metrics(df_cv)\n",
    "        mape = df_p['mape'].mean()\n",
    "        store_errors[store] = mape\n",
    "    except Exception as e:\n",
    "        print(f\"Store {store}: error during cross-validation - {e}\")\n",
    "        store_errors[store] = None\n",
    "\n",
    "results_df = pd.DataFrame(list(store_errors.items()), columns=['Store', 'MAPE'])\n",
    "results_df = results_df.dropna().sort_values('MAPE');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3444b-5356-4219-8f1e-8da8540d925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(results_df['Store'].astype(str), results_df['MAPE'])\n",
    "plt.xlabel('Store')\n",
    "plt.ylabel('Mean Absolute Percentage Error (MAPE)')\n",
    "plt.title('Prophet Forecasting Accuracy by Store')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7665b-bf94-4b6f-a798-8bc99e55f6aa",
   "metadata": {},
   "source": [
    "<p><b>Figure 8:</b> Barplot of MAPE scores based on cross validation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5beb4b-a625-47fa-a1ef-91f19e865254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "mape_values = results_df[['MAPE']].copy()\n",
    "store_labels = results_df['Store'].astype(str).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mape_scaled = scaler.fit_transform(mape_values)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(mape_scaled)\n",
    "results_df['Cluster'] = clusters\n",
    "\n",
    "k_palette = [\"#f032e6\", \"#f58231\", \"#4363d8\"]\n",
    "\n",
    "cluster_ranges = results_df.groupby('Cluster')['MAPE'].agg(['min', 'max']).sort_values(by='min')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i, (cluster_id, row) in enumerate(cluster_ranges.iterrows()):\n",
    "    plt.axhspan(\n",
    "        row['min'], row['max'],\n",
    "        facecolor=k_palette[cluster_id], \n",
    "        alpha=0.1, \n",
    "        label=f'Cluster {cluster_id}'\n",
    "    )\n",
    "\n",
    "sns.scatterplot(data=results_df, x='Store', y='MAPE', hue='Cluster', palette=k_palette, s=100)\n",
    "\n",
    "for i in range(results_df.shape[0]):\n",
    "    plt.text(\n",
    "        x=results_df['Store'].iloc[i], \n",
    "        y=results_df['MAPE'].iloc[i] + 0.002,\n",
    "        s=str(results_df['Store'].iloc[i]), \n",
    "        fontsize=8, \n",
    "        ha='center', \n",
    "        va='bottom'\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Store Forecasting MAPE Clusters with Background Regions')\n",
    "plt.ylabel('MAPE')\n",
    "plt.xlabel('Store')\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03baac23-5b55-4d0d-8d7f-09076769bc51",
   "metadata": {},
   "source": [
    "<p><b>Figure 9:</b> Scatter plot of store forecast performance groupings based on MAPE score.</p>\n",
    "\n",
    "Before forecasting, cross validation of the prophet model was used to assess performance on unseen data to ensure the model avoided overfitting. Figures 8 and 9 provide the cross validation results where error scores ranged between 0.02 and 0.1 which respectively mean 2% and 10%. Thus implying the forecast error is generally low as where a typical standard is: 0%-10% high accuracy, 11%-20% good accuracy and 21%-30% being acceptable. However, stores within the group 1 cluster may require additional features to improve forecast accuracy due to having the highest amount of error. Forecasting accuracy was based on a 13week (3 months) period into the future, an extened period of forecasting would require more data to maintain accuracy which will be evidenced by the forecast plots. However, overall this implied the forecasts generated by the model are highly accurate evidencing seasonality alone may be enough to explain weekly sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb83d0-fc4c-42b0-bf0b-86c8f7a38162",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 13  \n",
    "store_forecasts = {}\n",
    "store_anomalies = {}\n",
    "\n",
    "stores = sorted(stores)  \n",
    "\n",
    "plots_per_fig = 9\n",
    "rows, cols = 3, 3\n",
    "\n",
    "for idx, store in enumerate(stores):\n",
    "    df = store_dfs[store].copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'], dayfirst=True, errors='coerce')\n",
    "    df = df.dropna(subset=['ds', 'y'])\n",
    "\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        changepoint_prior_scale=0.5\n",
    "    )\n",
    "    model.fit(df)\n",
    "\n",
    "    future = model.make_future_dataframe(periods=forecast_horizon, freq='W')\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    forecast = forecast.merge(df[['ds', 'y']], on='ds', how='left')\n",
    "    forecast.rename(columns={'y': 'fact'}, inplace=True)\n",
    "    forecast['anomaly'] = 0\n",
    "    forecast.loc[forecast['fact'] > forecast['yhat_upper'], 'anomaly'] = 1\n",
    "    forecast.loc[forecast['fact'] < forecast['yhat_lower'], 'anomaly'] = -1\n",
    "    forecast['importance'] = 0.0\n",
    "    forecast.loc[forecast['anomaly'] == 1, 'importance'] = (\n",
    "        (forecast['fact'] - forecast['yhat_upper']) / forecast['fact']\n",
    "    )\n",
    "    forecast.loc[forecast['anomaly'] == -1, 'importance'] = (\n",
    "        (forecast['yhat_lower'] - forecast['fact']) / forecast['fact']\n",
    "    )\n",
    "\n",
    "    store_forecasts[store] = forecast\n",
    "    store_anomalies[store] = forecast[forecast['anomaly'] != 0]\n",
    "\n",
    "    if idx % plots_per_fig == 0:\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    ax = axes[idx % plots_per_fig]\n",
    "    \n",
    "    ax.plot(df['ds'], df['y'], label='Actual', color='black', linewidth=1)\n",
    "    ax.plot(forecast['ds'], forecast['yhat'], label='Forecast', color='#f032e6', linewidth=2)\n",
    "    ax.plot(forecast['ds'][-forecast_horizon:], forecast['yhat'][-forecast_horizon:], label='Forecast Horizon', color='purple', linewidth=3)\n",
    "    ax.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], color='grey', alpha=0.2)\n",
    "\n",
    "    anomalies = forecast[forecast['anomaly'] != 0]\n",
    "    ax.scatter(anomalies['ds'], anomalies['fact'], color='red', s=40, label='Anomaly', zorder=5)\n",
    "\n",
    "    ax.set_title(f'Store {store}', fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "    if idx % plots_per_fig == plots_per_fig - 1 or store == stores[-1]:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc='upper left', ncol=4)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.suptitle('Walmart Store Weekly Sale Forecasts with Anomalies', fontsize=16)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53583049-a47c-43f4-bab7-ded799396ad2",
   "metadata": {},
   "source": [
    "<p><b>Figures 10-14:</b> Lineplots for stores 1-45 showing model uncertainty and forecast prediction whilst identifying potential anomalies.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2d90e-cc3d-4daa-9c55-aefa4d421ec6",
   "metadata": {},
   "source": [
    "Figures 10-14 provide the forecast predictions and associated uncertainty for each store where red points indicate potential anomalies. However, skepticism should be employed for anomalous points as they are based on if they fall outside the models expectations. Although, anomalous points that fall far from the expected range are more probable to be correctly identified as many such points occur during the Christmas period where an anticipated dramatic increase in sales is expected. Due to a lack of data other extremes are more difficult to interpret and could correlate to a number of events such as other holiday periods or store closures. Furthermore, forecasting accuracy varried at different points of the horizon in particular during the future Christmas period and the endpoint of the horizon, this likely stems from the models difficulty in mapping Christmas peaks and dwindling data to predict further into the future. This evidences why a period of 13 weeks was chosen as a further forecast would likely bring greater uncertainty reducing the models forecasting accuracy. However, if additional data was available a forecasting for 6 months or 1 year may have been possible without experiencing decreases in forecast accuracy. Additionally, stores with more ambigous trends such as store 43 implied greater difficulty in accurate forecasts suggesting seasonality alone may not be the sole driver for some stores and would require further investigation to identify features which may impact weekly sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c811383-d8b0-4faa-8ffd-bc098c1e78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "store_id = 1\n",
    "store_df = Full_Walmart[Full_Walmart['Store'] == store_id].copy()\n",
    "\n",
    "store_df.set_index('Date', inplace=True)\n",
    "store_df.sort_index(inplace=True)\n",
    "store_sales = store_df['Weekly_Sales']\n",
    "\n",
    "decomposition = seasonal_decompose(store_sales, model='additive', period=52) \n",
    "\n",
    "decomposition.plot()\n",
    "plt.suptitle(f\"Time Series Decomposition for Store {store_id}\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e7df7-fe61-4717-8c29-5774bf532d53",
   "metadata": {},
   "source": [
    "<p><b>Figure 15:</b> Time series decomposition for store 1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01076b1c-7fa7-4996-a449-f6d52bba2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_id = 43\n",
    "store_df = Full_Walmart[Full_Walmart['Store'] == store_id].copy()\n",
    "\n",
    "store_df.set_index('Date', inplace=True)\n",
    "store_df.sort_index(inplace=True)\n",
    "store_sales = store_df['Weekly_Sales']\n",
    "\n",
    "decomposition = seasonal_decompose(store_sales, model='additive', period=52) \n",
    "\n",
    "decomposition.plot()\n",
    "plt.suptitle(f\"Time Series Decomposition for Store {store_id}\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5084f-07c7-44e0-a229-ea6750c5a1bd",
   "metadata": {},
   "source": [
    "<p><b>Figure 16:</b> Time series decomposition for store 43.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f8940-1e92-4dc9-a976-d3019e5c5062",
   "metadata": {},
   "source": [
    "To further understand forecasting accuracy STL (seasonal trend using LOESS) decompositions were produced. Overall, seasonal trend generally follows the original data from the examples. However residual clustering outside of the summer period, particularly for store 43, implied the presence of external factors aside from seasonality which impacted weekly sales. This in turn evidences the use of anomaly detection and strengthens the argument for further investigation into these periods or into specific stores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e061af-12d2-4b5d-8a02-16cbe0f36438",
   "metadata": {},
   "source": [
    "## Summary and Future Suggestions\n",
    "Overall, the results of the forecasting gave insight into store trends and patterns where seasonality was a dominant factor in driving weekly  sales. However, there is evidence of additional features at play such as holidays in the case of the Christmas period and other unknown features, which warrant further investigation. Additionally, although all forecasts are highly accurate and most falling bellow a 6% error margin group 1 MAPE stores could still benefit from additional features being added to further imporve accuracy particularly for stores with unique trends.\n",
    "\n",
    "However as evidenced, an investigation could take place to identify other drivers for weekly sales some suggestions include: promotion, internal store/supply issues and store closures to which, this information could improve the models capabilities and provide insight to determining the causes for anomalous points to reduce uncertainty. Additionally, some further non store related features could include: population demography, rural/urban locale (population counts), crime rates and median population income. Where these features could provide insights into weekly sales figures such as for example, differences could be expected in sale volumes between rural and urban locales as a result of population density. Additionally, this applies to median population income where typically areas with higher incomes would likely see more sales due to greater disposable income. \n",
    "\n",
    "As a final note, an additional use for this data type could be used to predict potential store performance for new stores. For example as used previously, dividing stores into performance groups, and using that as a replacement for weekly sales, could allow a classification algorithm to place new stores into performance categories based on a number of features. Where this method could provide insight when deciding between new store locations.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
